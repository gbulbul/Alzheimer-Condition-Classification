import os
import time
import shutil
import pathlib
import itertools
import glob

# import data handling tools
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras import regularizers
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SeparableConv2D
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense

dataset_dir = 'C:/Users/Alzheimer_dataset'
categories = ['Demented', 'NonDemented']
# Load images and labels into separate lists
image_paths = []
labels = []

for category in categories:
    for split in ['train', 'test']:
        category_dir = os.path.join(dataset_dir, split, category)
        for image_name in os.listdir(category_dir):
            image_path = os.path.join(category_dir, image_name)
            image_paths.append(image_path)
            labels.append(category)

# Create a DataFrame with image paths and labels
data = pd.DataFrame({'image_path': image_paths, 'label': labels})

# Split the data into train and test DataFrames
train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)

# Reset the index of the DataFrames
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

train_datagen = ImageDataGenerator(rescale=1./255,
    validation_split=0.2)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Define the batch size and target size
batch_size =16
target_size = (224, 224)

# Define the train and test generators
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col='image_path',
    y_col='label',
    class_mode='categorical',
    shuffle=True,
    batch_size=batch_size,
    target_size=target_size,
    subset='training'
)

test_generator = val_test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='label',
    shuffle=False,
    class_mode='categorical',
    batch_size=batch_size,
    target_size=target_size
)
# Create Model Structure
img_size = target_size
channels = 3
img_shape = img_size+(channels,)
class_count = len(list(train_generator.class_indices.keys())) # to define number of classes in dense layer
early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)
scores_model=[]

##########ALEXNET####################
class AlexNet(Sequential):
    def __init__(self, input_shape,num_classes):
        super().__init__()

        self.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), padding="valid", activation="relu", input_shape=input_shape))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())

        self.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())

        self.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Flatten())
        self.add(Dense(4096, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(4096, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(256, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(num_classes, activation="softmax"))

        self.summary()
        self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "img_shape":img_shape,
"loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"activation_sigmoid": 'sigmoid',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
num_class=4
model = AlexNet(img_shape, num_class)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, 
             callbacks = [early_stopping_callbacks] )
scores_model.append({'Model': 'AlexNet ','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)


###########RESNET##########
from tensorflow.keras.applications import resnet_v2
transfer_net = resnet_v2.ResNet50V2(weights ='imagenet', include_top = False, input_shape = img_shape)
transfer_net.trainable = False
class ResNet(Sequential):
     def __init__(self,params,num_class):
         super().__init__()
         self.add(transfer_net)
         self.add(Flatten())
         self.add(Dense(512, activation = params['activation_relu']))
         self.add(Dropout(0.5))
         self.add(Dense(num_class, activation = params['activation_sigmoid']))
         self.summary()
         self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "img_shape":img_shape,
"loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"activation_sigmoid": 'sigmoid',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
num_class=4
model=ResNet(params,num_class)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, callbacks = [early_stopping_callbacks]
                   )
scores_model.append({'Model': 'Resnet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)

preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)
########################CNN######################
class simple_CNN(Sequential):
    def __init__(self,params,num_class):
        super().__init__()
        self.add(layers.Conv2D(32, (3, 3), activation=params['activation'], input_shape= params["img_shape"]))
        self.add(layers.MaxPooling2D((2, 2)))
        self.add(layers.Conv2D(64, (3, 3), activation=params['activation']))
        self.add(layers.MaxPooling2D((2, 2)))
        self.add(layers.Conv2D(64, (3, 3), activation=params['activation']))
        self.add(layers.Flatten())
        self.add(layers.Dense(512, activation=params['activation'])),
        self.add(layers.Dense(num_class,activation=params['activation']))
        self.summary()
        self.compile(optimizer=params['optimizer'],
              loss=params['loss'],
              metrics=params['metric'])

params={
        "img_shape":img_shape,
"loss": 'categorical_crossentropy',
"optimizer": "adam",
"activation": 'relu',
"activation_softmax":'softmax',
"batch_size": 128,
"metric":'accuracy',
}
num_class=4
model=simple_CNN(params,num_class)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, callbacks = [early_stopping_callbacks]
                   )
scores_model.append({'Model': 'CNN','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

################CancerNET#############
class CancerNet(Sequential):
    def __init__(self,img_shape,classes):
        super().__init__()
        self.add(SeparableConv2D(32, (3, 3), padding="same", input_shape=img_shape))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(SeparableConv2D(64, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(64, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(Flatten())
        self.add(Dense(256))
        self.add(Activation("relu"))
        self.add(BatchNormalization())
        self.add(Dropout(0.5))

        self.add(Dense(classes))
        self.add(Activation("softmax"))
        self.summary()
        self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "img_shape":img_shape,
"loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
class_num=4
model = CancerNet(img_shape,class_num)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, steps_per_epoch = 80,
             callbacks = [early_stopping_callbacks] )
scores_model.append({'Model': 'CancerNet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

######EfficientNet###############
base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= "imagenet", input_shape= img_shape, pooling= 'max')
# base_model.trainable = False
class EfficientNet(Sequential):
    def __init__(self, input_shape,num_classes):
        super().__init__()

        self.add(base_model),
        self.add(BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001)),
        self.add(Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),
                bias_regularizer= regularizers.l1(0.006), activation= 'relu')),
        self.add(Dropout(rate= 0.45, seed= 123)),
        self.add(Dense(class_count, activation= 'softmax'))
        self.summary()
        self.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

model = EfficientNet(img_shape, num_classes = 4)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, 
             callbacks = [early_stopping_callbacks] )
scores_model.append({'Model': 'EfficientNet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

#####VGG#####
from tensorflow.keras.optimizers import RMSprop
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.callbacks import EarlyStopping
base_model = VGG16(
    weights='imagenet',
    include_top=False, 
    input_shape=img_size + (3,)
)
class VGG16(Sequential):
    def __init__(self, input_shape,num_classes):
        super().__init__()

        self.add(base_model),
        self.add(layers.Flatten()),
        self.add(layers.Dropout(0.5)),
        self.add(layers.Dense(num_classes, activation='sigmoid'))

        self.layers[0].trainable = False
        self.summary()
        self.compile(
    loss='binary_crossentropy',
    optimizer=keras.optimizers.RMSprop(lr=1e-4),
    metrics=['accuracy']
)

model = VGG16(img_shape, num_classes = 4)
history = model.fit(train_generator,
    validation_data=test_generator, epochs=10, 
             callbacks = [early_stopping_callbacks] )
scores_model.append({'Model': 'VGG16','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

####XCEPTION ##### 
base_model = tf.keras.applications.Xception(include_top= False, weights= "imagenet", input_shape= img_shape, pooling= 'max',)

class XCEPTION(Sequential):
    def __init__(self, input_shape,num_classes):
        super().__init__()

        self.add(base_model),
        self.add(layers.Dense(512, activation='relu')),
        self.add(layers.Dropout(0.2)),
        self.add(layers.Dense(num_classes, activation='sigmoid')),
        self.summary()
        self.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

model = XCEPTION(img_shape, num_classes = 4)
history = model.fit(train_generator,
    validation_data=test_generator,
    epochs=1,
    steps_per_epoch = 2,
    callbacks = [early_stopping_callbacks])
plot_training(history)
scores_model = []
scores_model.append({'Model': 'Xception','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

